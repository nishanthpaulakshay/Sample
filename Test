1. Normality in Vector
## Generate two data sets
## First Normal, second from a t-distribution
words1 = rnorm(100); words2 = rt(100, df=3)

## Have a look at the densities
plot(density(words1));plot(density(words2))

## Perform the test
shapiro.test(words1); shapiro.test(words2)

## Plot using a qqplot
qqnorm(words1);qqline(words1, col = 2)
qqnorm(words2);qqline(words2, col = 2)


2. 
Why is the normal distribution important?

Many (but not all!) variables in fields such as psychology tend to have normally distributed scores.
The fairly widespread occurrence of normally distributed scores may be due to random processes that tend to generate normal distributions.
The sampling distribution for statistics such as M (sample mean) tend to be normally distributed (even when the distribution of individual X scores is not perfectly normal). 
This is based on the Central Limit Theorem, which was developed using imaginary normally distributed scores. Monte Carlo simulations show that even when assumptions about distribution shape for individual scores are violated, sampling distributions may still be fairly normal (Sawilowsky & Blair, 1992.) However, there are limits to this robustness against violations of assumptions.

The shape of the sampling distribution is normal for statistics such as M when population standard deviation sigma is known and other assumptions are met. The shape of the distribution is modified normal; i.e., a t distribution, when population standard deviation sigma is not known and sample standard deviation SD is used to estimate it. The sampling distribution is the basis for construction of statistical significance tests and Confidence Intervals (in other words, the statistical techniques taught in most introductory courses and widely relied on in research reports).
However, keep in mind that not all X variables have normally distributed scores; and that not all sample statistics have normally shaped sampling distributions. In these situations, different analyses not taught in introductory courses are often needed, and tools such as bootstrapping are sometimes used.

3.
Cost Function : A cost function is something you want to minimize. For example, your cost function might be the sum of squared errors 
over your training set. Gradient descent is a method for finding the minimum of a function of multiple variables. So you can use gradient
descent to minimize your cost function. If your cost is a function of K variables, then the gradient is the length-K vector that 
defines the direction in which the cost is increasing most rapidly. So in gradient descent, you follow the negative of the gradient 
to the point where the cost is a minimum. If someone is talking about gradient descent in a machine learning context, the cost function
is probably implied (it is the function to which you are applying the gradient descent algorithm).

Learning function: The most common type of machine learning is to learn the mapping Y=f(X) to make predictions of Y for new X.

This is called predictive modeling or predictive analytics and our goal is to make the most accurate predictions possible.

As such, we are not really interested in the shape and form of the function (f) that we are learning, only that it makes accurate 
predictions.

We could learn the mapping of Y=f(X) to learn more about the relationship in the data and this is called statistical inference. 
If this were the goal, we would use simpler methods and value understanding the learned model and form of (f) above making accurate 
predictions.

When we learn a function (f) we are estimating its form from the data that we have available. As such, this estimate will have error.
It will not be a perfect estimate for the underlying hypothetical best mapping from Y given X.

Much time in applied machine learning is spent attempting to improve the estimate of the underlying function and in term improve the 
performance of the predictions made by the model.

Essentially, the terms "classifier" and "model" are synonymous in certain contexts; however, sometimes people refer to "classifier" as the learning algorithm that learns the model from the training data. To makes things more tractable, let's define some of the key terminology:

Training sample: A training sample is a data point x in an available training set that we use for tackling a predictive modeling task. For example, if we are interested in classifying emails, one email in our dataset would be one training sample. Sometimes, people also use the synonymous terms training instance or training example.
Target function: In predictive modeling, we are typically interested in modeling a particular process; we want to learn or approximate a particular function that, for example, let's us distinguish spam from non-spam email. The target function f(x) = y is the true function f that we want to model.

Hypothesis: A hypothesis is a certain function that we believe (or hope) is similar to the true function, the target function that we want to model. In context of email spam classification, it would be the rule we came up with that allows us to separate spam from non-spam emails.
Model: In machine learning field, the terms hypothesis and model are often used interchangeably. In other sciences, they can have different meanings, i.e., the hypothesis would be the "educated guess" by the scientist, and the model would be the manifestation of this guess that can be used to test the hypothesis.
Learning algorithm: Again, our goal is to find or approximate the target function, and the learning algorithm is a set of instructions that tries to model the target function using our training dataset. A learning algorithm comes with a hypothesis space, the set of possible hypotheses it can come up with in order to model the unknown target function by formulating the final hypothesis
Classifier: A classifier is a special case of a hypothesis (nowadays, often learned by a machine learning algorithm). A classifier is a hypothesis or discrete-valued function that is used to assign (categorical) class labels to particular data points. In the email classification example, this classifier could be a hypothesis for labeling emails as spam or non-spam. However, a hypothesis must not necessarily be synonymous to a classifier. In a different application, our hypothesis could be a function for mapping study time and educational backgrounds of students to their future SAT scores.
